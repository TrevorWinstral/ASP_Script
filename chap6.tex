\chapter{Standard Poisson Process}
In discrete time processes $(X_n)_{n\in N}$, the law is characterised by the law of $(X_{n_1},..X_{n_k}; n_1 \ldots n_k \in \mathbb{N})$. In continuous time processes we have $(X_t)_{t\geq 0}$, we need to define $X_t$ for every $ t \in \mathbb{R}$ which is not countable.

\noindent \textbf{Outset} We would like to define a renewal process which also fulfills the Markov property. Furthermore we would like a simple continuous time process which is in some way a 'universal' stationary process on $\mathbb{R}_+ \to \mathbb{N}$ with independent increments and jumps of size 1. We would also like to see if any of the ideas from the previous chapter can be specified to this context.

\textbf{Applications} Queuing processes, insurance claims, compound Poisson process.

\textbf{Framework} $(\Omega, \mathcal{F}, \mathbb{P})$ probability space, time space: $\mathbb{R}_{+}=[0,\infty)$ 

There are 2 points of view: random points on $\mathbb{R}_{+}$ (reminiscent of ppp) or continuous time stochastic process (renewal process).

\section{Exponential Random Variables}
\textbf{Note} We will use the 2nd point of view here.

\begin{defn}
	Let $\lambda> 0$, a real random variable $T$ is exponential with parameter $\lambda$ (we write $T \sim Exp(\lambda)$) if it has density $f(t) = \lambda e ^{-\lambda t} \mathbbm{1}_{\{t\geq 0\}}$ equivalently for all $t\geq 0\ \mathbb{P}_{} \left[ T>t \right] = e^{-\lambda t}$
\end{defn}

\begin{prop}[Memoryless Property]
	Let $\lambda > 0$ and $T$ be an Exp$(\lambda)$ random variable. Then  
	\begin{align}
		\boxed{\forall s,t\geq 0\quad \mathbb{P}_{} \left[ T>s+t | T>t \right] = \mathbb{P}_{} \left[ T>s \right]. }
	\end{align}
\end{prop}
\begin{prop}[Minimum of independent Exponentials]
	Let $n\geq 0, T_1, \ldots ,T_n$ independent with $T_i \sim Exp(\lambda_i)$ for $ \lambda_i > 0$, then 
\begin{itemize}
	\item $\min\{T_1, \ldots, T_n\} \sim \textrm{Exp}(\lambda_1+ \ldots +\lambda_n)$,
	\item $\mathbb{P}_{} \left[ T_1 = \min\{T_1, \ldots ,T_n\} \right] = \frac{\lambda_1}{\lambda_1+ \ldots +\lambda_n}$.
\end{itemize}

\end{prop}
 
\textbf{Reminder} $X$ a real random variable with density $f$, $Y$ a random variable with values in some measurable space $E$ independent of X. Then for every $\phi:\mathbb{R} \times E \to \mathbb{R}$ measurable and bounded we have  
\begin{align}
	\mathbb{E}_{} \left[ \phi(X,Y) \right] = \int_{0}^{\infty} \mathbb{E}_{} \left[ \phi(x,Y) \right] f(x) dx.
\end{align}
\begin{proof}
For every $t\geq 0$ 
\begin{align}
	\mathbb{P}_{} \left[ \min(T_1, \ldots , T_n) \geq t \right] = \prod_{i=1}^{n}\mathbb{P}_{} \left[ T_i \geq t \right] = \exp(-(\lambda _1 + \ldots + \lambda_n)t).
\end{align}
\begin{align}
	\mathbb{P}_{} \left[ T_1 = \min(T_1,\ldots ,T_n) \right] &= \int_{0}^{\infty } \mathbb{P}_{} \left[ t = \min(t, T_2, \ldots ,T)n) \right] \lambda _{1}e^{-\lambda_1 t} dt \\
								 &= \int_{0}^{\infty } \mathbb{P}_{} \left[ \min(T_2, \ldots ,T_n) \geq t \right] \lambda _1 e^{-\lambda t}dt \\
								 &= \lambda_1 \int_{0}^{\infty } e^{-(\lambda_1 + \ldots + \lambda_n)t}dt = \frac{\lambda_1}{\lambda_1 + \ldots + \lambda_n}.
\end{align}
\end{proof}


\begin{prop}[Sum of Exponentials]
	Let $\lambda > 0,\ n\geq 1$. Let $T_1, \ldots, T_n$ be i.i.d. Exp$(\lambda)$ random variables. Then  $S_n = T_1+ \ldots +T_n$ is  $\Gamma(n, \lambda)$ distributed. I.e. $S_n$ is continuous with density $f_{S_n}(t)=\lambda e^{-\lambda t} \frac{(\lambda t)^{n-1}}{(n-1)!}$
\end{prop}
\begin{rmk}[]
We can check that $\Gamma(1,t)=\textrm{Exp}(\lambda)$
\end{rmk}

\begin{proof}
	By induction.

	$n=1$ is trivial.

	Assume that the results holds for some $n\geq 1$. Let $T_1,\ldots ,T_{n+1}$ be i.i.d. Exp$(\lambda)$. $S_n = T_1 + \ldots + T_n$ and $T_{n+1}$ are independent. Hence, $S_{n+1} = S_n + T_{n+1}$ and $T_{n+1}$ admits a density given by the convolution
	\begin{align}
		f_{S_{n+1}} &= \int_{0}^{t} f_{S_n}(s)f_{T_{n+1}}(t-s)ds = \int_{0}^{t} \lambda^2 e^{-\lambda s} \frac{(\lambda s)^{n-1}}{(n-1)!} e^{-\lambda(t-s)}ds = \lambda e^{-\lambda t} \frac{(\lambda t)^{n}}{n!}.
	\end{align}
\end{proof}


\section{Definition of Poisson Processes}
\textbf{Setup}  $\lambda > 0, (T_i)_{i\geq 0}$ i.i.d. Exp$(\lambda ), S_n = T_1+ \ldots +T_n$

\begin{defn}
	The stochastic process $N=(N_t)_{t\geq 0}$
	\begin{align}
		\boxed{\forall t\geq 0 \quad N_t = \sum_{i=1}^{\infty} \mathbbm{1}_{S_i \leq t}.}
	\end{align}
	is called the \emph{Poisson process with intensity $\lambda$} (pp$(\lambda)$). The random variables $T_1,T_2, \ldots $ are the inter-arrival times and  $S_1,S_2, \ldots $ the arrival/jump times.
\end{defn}
{\color{blue}
\begin{rmk}[]
	Thus the Poisson process with intensity $\lambda $ is a renewal process with arrival distribution Exp$(\lambda)$.
\end{rmk}}

\noindent
\textbf{Elementary Properties}
\begin{itemize}
	\item The mapping $t \mapsto N_t$ is a.s. right continuous, with values in $\mathbb{N}$,
	\item For fixed $t\geq 0$, $N_t$ has distribution Pois$(\lambda t)$, i.e. $\mathbb{P}_{} \left[ N_t = n \right] = \frac{(\lambda t)^n}{n!}e^{- \lambda  t}$.
\end{itemize}

{\color{blue}
\noindent \textbf{Comment} "A property hold a.s." means there exists a measurable set $A$ with $\mathbb{P}_{} \left[ A \right] =1$ and such that for every $\omega \in A$ the property holds. This is different than what we have previously used for almost sure, and is a necessary change as properties like continuity are not measurable.

\begin{rmk}[]
	We could have also defined the Poisson process as a Poisson point process on $\mathbb{R}_+$ with intensity measure $\lambda \mathcal{L}$. As we can see in the following proposition.
\end{rmk}
\begin{prop}[]
	The following are equivalent
\begin{enumerate}
	\item $N$ is a pp$(\lambda)$,
	\item $N$ is a ppp$(\lambda \mathcal{L})$ ($\mathbb{R}_+$).
\end{enumerate}
\end{prop}
// Maybe this is nicer with the Laplace functional? //
\begin{proof}
	'$\Longrightarrow$' We may need to say that when we write $N_t$ we mean $N([0,t])$ to show how this is a random measure. Next it is clear that $N(B)$ is a Pois$(\lambda \mathcal{L}(B))$ random variable. Then we just have to show independence of disjoint intervals (Dynkin). 

	We need to show that for disjoint measurable sets $B_1,\ldots ,B_k$ that the $N(B_i)$ are independent. We will only show this for $k=2$. Furthermore we will only show for $B_1 =(t,t+h)$ and $B_2=(s,s+H)$ as such intervals form a $\pi $-system generating the entire Borel $\sigma $-algebra. $N((t,t+h)) = N_{t+h}-N_{t}$
\end{proof}


}

\section{Markov Property}
\begin{theorem}[Markov Property of N]
	Fix $t\geq 0$, the stochastic process $N^{(t)}=(N^{(t)}_{s})_{s \geq 0}$ defined by $N^{(t)}_s = N_{t+s}-N_{t}$ is a Poisson process, independent of $(N_u)_{0 \leq u \leq t}$.
\end{theorem}
{\color{blue}
\begin{proof}
	Since N is a ppp$(\lambda \mathcal{L})$, $N([0,t])$ is independent of $N((t,t+s])$. To use that this is a ppp we already had to prove they are independent?	
\end{proof}}


\section{Stationary and Independent Increments}
{\color{blue}
\textbf{Motivation} We want to describe the law of $(N_{t_0}, \ldots ,N_{t_k})$, the key here is that they are not totally independent. If we have 5 points at time $t_0$ then we know at time $t_1$ there will be at least 5 points. So we look at the law of $(N_{t_1}-N_{t_0}, \ldots ,N_{t_k}-N_{t_{k-1}})$ i.e. the increments.}

\begin{defn}
	A stochastic process $(X_t)_{t\geq 0}$ is said to have \emph{independent and stationary increments} if 
	\begin{align}
		\forall k \geq 1,\  \forall 0=t_0 <  \ldots  < t_k \quad  X_{t_1}-X_{t_0},  \ldots , X_{t_k}- X _{t_{k-1}} \textrm{ are independent,}
		\end{align}
		and
		\begin{align}
		\forall  s<t,\ \forall  n \geq 0 \quad X_t - X_s \stackrel{\textrm{law}}{=} X_{t+h}-X_{s+h}.
	\end{align}
\end{defn}

\begin{theorem}[Stationary and Independent Increments]
We have the following
\begin{enumerate}
	\item For all $k \geq 1$ and every $0=t_0< \ldots <t_k$ we have
		\begin{align}
		N_{t_1}-N_{t_0}, \ldots ,N_{t_k}- N_{t_{k-1}} \textrm{ are independent,}
		\end{align}
	\item For all $s \leq t$  
		\begin{align}
			N_t - N_s \sim \textrm{Pois}(\lambda (t-s)).
		\end{align}
\end{enumerate}
In particular $N=(N_t)_{t\geq 0}$ has independent and stationary increments.
\end{theorem}
{\color{blue} // Do you prefer how it looks in the theorem or in the definition?//}
\begin{rmk}[]
The statements of the theorem are equivalent to for all $0=t_0 < \ldots < t_k $ and every $m_1, \ldots ,m_k \in \mathbb{N}$ 
\begin{align}
	&	\mathbb{P}_{} \left[ N_{t_1}=m_1, N_{t_2}-N_{t_1}=m_2 - m_1, \ldots , N_{t_k}-N_{t_{k-1}}=m_k - m_{k-1} \right] \\ 
	&\qquad \stackrel{(*)}{=} \prod_{i=1}^{k}\frac{(\lambda (t_i - t_{i-1}))^{m_i-m_{i-1}}}{m_i - m_{i-1}} e^{- \lambda (t_i - t_{i-1})}
\end{align}
\end{rmk}
\begin{proof}
	We will prove $(*)$ by induction on $k$. The case $k=1$ corresponds to $N_t \sim \textrm{Pois}(\lambda t)$. Now let $k\geq 1$ and assume that $(*)$ holds. Let $t_0<\ldots<t_{k+1}$ and  $n_1,\ldots ,_{k+1} \in \mathbb{N}$.
	\begin{align}
		&\mathbb{P}_{} \left[ \underbrace{N_{t_1} - N_{t_0} = n_1, \ldots ,}_{\in \sigma((N_{u})_{u\leq t_k )}} \underbrace{N_{t_{k+1}} - N_{t_k}=n_{k+1}}_{=N_{t_{k+1}-t_{k}}^{(t_k)}} \right]  \\
		&\qquad =\mathbb{P}_{} \left[ N_{t_1} - N_{t_0} = n_1, \ldots , N_{t_k} - N_{t_{k-1}}=n_k \right] \mathbb{P}_{} \left[ N_{t_{k+1} - t_k}=n_{k+1} \right] \\
		&\qquad =\prod_{i\leq k} \frac{[\lambda (t_i - t_{i-1})]^{n_i}}{n_i!}e^{-\lambda (t_i - t_{i-1})} \cdot \frac{[\lambda (t_{k+1} - t_k) ]^{n_{k+1}}}{n_{k+1}!} e^{\lambda (t_{k+1} - t_k)}\\ 
		&\qquad =\prod_{i\leq k+1} \frac{[\lambda (t_i - t_{i-1})]^{n_i}}{n_i!}e^{-\lambda (t_i - t_{i-1})}
	\end{align}

	
\end{proof}


\section{Finite Marginals Characterization}
\textbf{Motivation} If $N$ is a $ \textrm{pp} (\lambda )$, we know the law of the vector $(N_{t_1}, \ldots , N_{t_k})$ for every fixed $t_1 < \ldots < t_k$. These are called the finite marginal laws: they characterize the law of the stochastic process $(N_t)_{t\geq 0}$. Conversely if a stochastic process $(N_t)_{t\geq 0}$ has the same finite marginals as a $ \textrm{pp} (\lambda )$, is it a $ \textrm{pp} (\lambda )$?

\underline{No}, for $T_1, T_2, \ldots$ i.i.d. $ \textrm{Exp} (\lambda)$, consider the process $\widetilde{N}_{t} = \sum_{i> 0}^{} \mathbbm{1}_{S_i < t} $. This is not a Poisson process, because it does not have right-continuous trajectories almost surely. But it has the same finite marginals as the Poisson process defined by $N_t = \sum_{i> 0}^{}  \mathbbm{1}_{S_i \leq t} $.

One can even construct a 'worse' counter example (inspired by the Brownian Motion lecture notes by Prof. Werner). Let $N$ be a $ \textrm{pp} (\lambda )$. Let $(X_i)_{i \in \mathbb{N}}$ be i.i.d. $ \textrm{Exp} (\lambda )$ random variables independent of $N$. Notice that $\mathcal{X}= \{ X_1, X_2, \ldots \}$ is dense in $\mathbb{R}_+$. The process defined by $\widetilde{N}_{t} = N+t + \mathbbm{1}_{t\in \mathcal{X}} $ for all $t$. This process is not a $ \textrm{pp} (\lambda )$ (it is nowhere right continuous a.s.), but it has the same finite marginals.

\underline{Yes}, In this section we will see that if we add a regularity condition on $t \mapsto N_t$. Then it is characterized by its finite marginal laws. 

\noindent Recall the definition of a counting process.
\begin{defn}
	Let $N=(N_t)_{t \geq 0}$ be a continuous time stochastic process with values in $\mathbb{R}$. We say that $N$ is a \emph{counting process} if the following holds a.s.
\begin{enumerate}
	\item $N_0 = 0$ a.s.,
	\item  $t \mapsto N_t$ is non-decreasing, right continuous, with values in $\mathbb{N}$.
\end{enumerate}
In this case we can define the successive \emph{jump times} by setting $S_1=\min \{t:\ N_t>0\}$ and by induction for $i> 0$ $S_{i+1} = \min\{t \geq S_i: \ N_t > N_{S_i} \}$.
\end{defn}
{\color{blue}
\begin{ex}[]
	$ \textrm{pp}(\lambda )$ is a counting process.
\end{ex}}
\begin{rmk}[]
	$N$ is $ \textrm{pp} ( \lambda )$ if and only if $N$ is a counting process with jumps of size 1 (i.e. for all $t$ $\limsup_{h \to 0} N_t - N_{t-h} \leq 1$ a.s.) and $S_1, S_2 - S_1, S_3-S_2, \ldots$ are i.i.d. $ \textrm{Exp} (\lambda )$.
\end{rmk}

\begin{theorem}[]
	Let $\lambda> 0$. Let $N=(N_t)_{t\geq 0}$ be a counting process, the following are equivalent
\begin{enumerate}
	\item $N$ is $pp(\lambda)$
	\item $\forall k \geq 1,\ \forall 0=t_0  < \ldots <t_k,\ \forall n_1, \ldots ,n_k \in \mathbb{N}$ we have 
		\begin{align}
		\boxed{\mathbb{P}_{} \left[ N_{t_1}-N_{t_0}=n_1, \ldots ,N_{t_k}-N_{t_k-1}=n_k \right] = \prod_{i=1}^k \frac{(\lambda (t_i - t_{i-1}))^{n_i}}{n_i!} e^{-\lambda (t_i - t_{i-1})}.} 
		\end{align}
\end{enumerate}
\end{theorem}
\begin{proof}
	'$\Longrightarrow$' we have already seen.

'$\Longleftarrow$' We first prove that $N$ has jumps of size 1 on every segment $[0,A]$ for $A>0$. Let $E_n = \{ \forall i \leq n:\ N_{\frac{iA}{n}} - N_{\frac{(i+1)A}{n}} \}$ for $n> 0$. We have 
\begin{align}
	\mathbb{P}_{} \left[ E_n \right] = \prod_{i\leq n}(e^{-\frac{A}{n}} + e^{-\frac{A}{n}}\frac{A}{n}) = e^{-A}(1+\frac{A}{n})^{n} \to 1.
\end{align}
Let $E = \bigcap_{n> 0}E_n$. We have $\mathbb{P}_{} \left[ E \right]  =1$ (because $\mathbb{P}_{} \left[ E \right]  \geq \mathbb{P}_{} \left[ E_n \right] $ for all $n> 0$ ) and furthermore for all $\omega \in E$ 
\begin{align}
	\forall t \leq A \limsup_{s \to 0} N_t(\omega) - N_{t-s}(\omega) \leq 1.
\end{align}
This concludes that $N$ has jumps of size 1.

Fix $k> 0$. We prove that $T_1=S_1, T_2 =S_2-S_1,\ldots$ are i.i.d. $ \textrm{Exp} (\lambda )$. We begin with the computation of the law of $(S_1,\ldots,S_k)$. Let $U = \{ (s_1, \ldots s_k)\in \mathbb{R}^{k}:\ 0 \leq s_1 \leq \ldots \leq s_k \}$. We now show that for all $H \in \mathcal{B}(U)$
\begin{align}
	\mathbb{P}_{} \left[ (S_1, \ldots, S_k ) \in H \right] = \int_{H}^{} \lambda^{k}e^{\lambda y_k}dy_1 \ldots dy_{k}.	
\end{align}
By Dynkin's lemma, it suffices to prove it for $H=[s,t_1) \times \ldots \times [s_k,t_k)$ where $s_1<t_1 < \ldots <s_k<t_k$ (by convention $t_0=0$).
\begin{align}
	\mathbb{P}_{} \left[ \forall i \leq k \ S_i \in [s_i,t_i) \right] 
	&= \mathbb{P}_{} \left[ \bigcap_{i\leq k}\{N_{s_i}-N_{t_{i+1}}=0 \} \cap \bigcap_{i<k}\{ N_{t_i} - N_{s_i} =1 \} \cap \{N_{t_k} - N_{s_k} \geq 1 \} \right] \\
	&= \prod_{i\leq k} e^{-\lambda (s_i - t_{i-1})} \cdot \prod_{i<k}\lambda(t_i - s_i) e^{-\lambda(s_i - t_i)} \cdot \left(1 - e^{-\lambda (t_k -s_k)}\right) \\
	&= \prod_{i<k}\lambda (t_i - s_i) e^{-\lambda s_k}\left(1-e^{-\lambda(t_k -s_k)}\right) \\
	&= \prod_{i<k}\int_{s_i}^{t_i} \lambda dy_i \cdot \int_{s_k}^{t_k} \lambda e^{-\lambda y_k}dy_k.
\end{align}
Hence $(S_1,\ldots,S_k)$ has density $f(y_1,\ldots,y_k)=\lambda^{k}e^{-\lambda y_k}\mathbbm{1}_{y_1 < \ldots y_k} $. Define the map $h(t_1,\ldots ,t_k) = (t_1, t_1+t_2,\ldots, t_1 + \ldots + t_k)$. This way we have $(T_1, \ldots , T_k)= h^{-1}((S_1, \ldots ,S_k))$. By change of variables (and using that the Jacobian of $h$ is 1), $(T_1, \ldots , T_k)$ admits the density
\begin{align}
	(f \circ h) (t_1, \ldots ,t_k) = \lambda^{k}e^{-\lambda(t_1+\ldots +t_k)} \mathbbm{1}_{t_1 < \ldots < t_1 +\ldots +t_k} = \prod_{i=1}^{k}\lambda e^{- \lambda t_i}\mathbbm{1}_{t_i>0} ,	
\end{align}
which establishes that $T_1,\ldots ,T_k$ are i.i.d. $ \textrm{Exp} (\lambda )$ random variables. Since $k$ was arbitrary, we conclude that the interarrival times $T_1, T_2, \ldots$ are i.i.d. $ \textrm{Exp} (\lambda)$.
\end{proof}

\section{Microscopic Characterization}
\noindent \textbf{Motivation} Droplets of water falling on the half line $\mathbb{R}_+$ {\color{blue}(TODO Figure)}. The random points on $\mathbb{R}_+$ are equal to the position at which the droplets have fallen. Define for every interval $I\subset \mathbb{R}_+$, $N(I)=\#\{ \textrm{random points in } I\}$. 

\underline{Hypotheses} 
\begin{enumerate}
	\item If $I_1,\ldots ,I_k$ are disjoint intervals, $N(I_1),\ldots,N(I_k)$ are independent,   
	\item If $I'=(a+h,b+h]$ is a translation of $I=(a,b]$ for $h\geq 0$, $N(I') \stackrel{ \textrm{(law)} }{=}N(I)$,
	\item For all bounded intervals $I\subset \mathbb{R}_+$, $N(I)\in \mathbb{N}$ a.s.
\end{enumerate}
The hypotheses imply that the stochastic process $N_t=N([0,t])$ is a counting process with independent and stationary increments. We know that such a process (the Poisson process) exists. Thus we are left with the question, is it the only one?

\underline{Yes}! But we need an addition condition fixing the {\color{blue}\st{density}intensity} $\lambda $.
\begin{theorem}[]
	Let $N$ be a counting process, $\lambda> 0$. The following are equivalent
\begin{enumerate}
	\item $N$ is $ \textrm{pp} (\lambda)$, 
	\item $N$ has independent and stationary increments and 
		\begin{align}
			\mathbb{P}_{} \left[ N_t =1 \right] &= \lambda t + o_{t\to 0}(t) \\
			\mathbb{P}_{} \left[ N_t \geq 2 \right] &= o_{t \to 0}(t).
		\end{align}
\end{enumerate}
\end{theorem}
\begin{rmk}[]
	The first equation means $\lim_{t\to0}\frac{\mathbb{P}_{} \left[ N_t =1 \right] }{\lambda t}=1$. The second equation means $\lim_{t\to 0} \frac{\mathbb{P}_{} \left[ N_t \geq 2 \right] }{t}=0$.
\end{rmk}
\begin{lemma}[]
	Let $(p_n)_{n> 0}$ be a sequence of parameters ($p_{n}\in [0,1]$) and $\lambda \in (0, \infty )$ such that
	\begin{align}
		\lim_{n\to \infty }np_n = \lambda .
	\end{align}
	For every $n$ let $X_n \sim  \textrm{Bin}(n,p_n) $. Then
	\begin{align}
		X_n \stackrel{ (\textrm{d}) }{\longrightarrow } \textrm{Pois} (\lambda ).
	\end{align}
\end{lemma}
\begin{proof}[Proof (Lemma)]
	See Probability Theory, p.47.
\end{proof}
\begin{proof}[Proof (Theorem)]
'$\Longrightarrow$' 
\begin{align}
	\mathbb{P}_{} \left[ N_t=1 \right] &= \lambda t e^{-\lambda t} = \lambda t + o(t),\\
	\mathbb{P}_{} \left[ N_t \geq 2 \right] &= 1 - e^{\lambda t} - \lambda t e^{-\lambda t} = o(t).
\end{align}
'$\Longleftarrow$'
We already have that $(N_t)$ has independent increments. It suffices to prove that 
\begin{align}
	\forall s<t \quad N_t - N_s \sim  \textrm{Pois} (\lambda (t-s)).
\end{align}
Since $N$ has stationary increments, it suffices to prove that
\begin{align}
	\forall t \quad N_t \sim  \textrm{Pois} (\lambda t).
\end{align}
Fix $t\in(0,\infty )$. Let $n> 0$. By independence and stationarity of the increments, the variables $Z_i^{(n)}=\mathbbm{1}_{N_{\frac{it}{n}}- N_{\frac{(i-1)t}{n}}\geq 1} $ are i.i.d. $ \textrm{Ber}(p_n)$ random variables, where $p_n = \mathbb{P}_{} \left[ N_{\frac{t}{n}}\geq 1 \right] = \lambda \frac{t}{n} + o\left(\frac{t}{n}\right)$. 
{\color{blue}TODO: Figure}
Hence $X_n = \sum_{i=1}^{n} Z_i^{(n)}$ is a $ \textrm{Bin } (n, p_n)$ random variable. Since $np_n \to \lambda t$, the lemma implies that for any $k \in \mathbb{N}$ 
\begin{align}
	\mathbb{P}_{} \left[ X_n =k \right] \to \frac{(\lambda t)^{k}}{k!}e^{\lambda t}.
\end{align}
We have for every $n> 0$ 
\begin{align}
	\mathbb{P}_{} \left[ N_t \neq X_n \right] &= \mathbb{P}_{} \left[ \bigcap_{1\leq i \leq n} \{ N_{\frac{it}{n}} - N_{\frac{(i-1)t}{n}}\leq 2\} \right] \leq \sum_{i=1}^{n} \mathbb{P}_{} \left[ N_{\frac{it}{n}} - N_{\frac{(i-1)t}{n}} \geq 2 \right] = n \mathbb{P}_{} \left[ N_{\frac{t}{n}}\geq 2 \right].
\end{align}
Since $\mathbb{P}_{} \left[ N_{\frac{t}{n}}\geq 2 \right] = o\left( \frac{t}{n}\right) $, we get that 
\begin{align}
	\lim_{n\to \infty }\mathbb{P}_{} \left[ N_t \neq X_n \right] =0.	
\end{align}
Fix $k\in \mathbb{N}$. For every $n> 0$ 
\begin{align}
	\left| \mathbb{P}_{} \left[ N_t =k \right] - \mathbb{P}_{} \left[ X_n =k \right] \right| \leq \mathbb{E}_{} \left[ \left| \mathbbm{1}_{N_t =k} - \mathbbm{1}_{X_n=k} \right| \right] \leq \mathbb{P}_{} \left[ N_t \neq X_n \right] .
\end{align}
Hence $\mathbb{P}_{} \left[ N_t =k \right] = \lim_{n\to\infty }\mathbb{P}_{} \left[ X_n = k \right] = \frac{(\lambda t)^{k}}{k!}e^{-\lambda k}$.
\end{proof}


\section{Properties of Poisson Process}

\begin{theorem}[Law of Large Numbers]
Let $N$ be a $ \textrm{pp}(\lambda)$, $\lambda > 0$, then 
\begin{align}
	\boxed{	\lim_{t \to \infty} \frac{N_t}{t}=\lambda.}
\end{align}
\end{theorem}
\begin{proof}
	{\color{blue}The Law of Large Numbers for Renewal processes applies, thus we are done.}
\end{proof}
\noindent
\textbf{Motivation} If we want to specify (and remove) certain points, for instance if the Poisson process is describing arrival times at a bakery then say we want to differentiate between customers who are younger than 45 and those who are older. If we just look at one of these groups, what type of process are they?

\begin{theorem}[Thinning]
	Let $(N_t)_{t\geq 0} \sim pp(\lambda)$ with jump times $(S_i)_{i\geq 0}$. Let $(X_n)_{n\geq 0}$ i.i.d. $Ber(p)$ independent of $N$ (this is called the marking of $N$). Define 
	\begin{align}
		N_t^1 &= \sum_{i\geq 1}^{} \mathbbm{1} _{S_i \leq t, X_i = 1},\\
		N_t^0 &= \sum_{i \geq 1}^{} \mathbbm{1}_{S_i \leq t, X_i = 0}.
	\end{align}
\\ \noindent	
	$(N_t^0)$ and  $(N_t^1)$ are independent Poisson processes with respective rates $\lambda_0 = (1-p)\lambda,\ \lambda _1=p\lambda $.
\end{theorem}
\begin{rmk}[]
	$N_t = N_{t}^{0}+N_{t}^{1}$ almost surely.
\end{rmk}
{\color{blue}
\begin{proof}[Proof (Sketch)]
	We will view $N$ as a Poisson point process ($N=\sum_{i}^{} \delta_{t_i}$), and then construct the process $M=\sum_{i}^{} \delta_{(t_i, \mathbbm{1}_{X_i= 1}) }$ as the marked ppp. This is a Poisson point process on $\mathbb{R}_+ \times \{0,1\}$, by restricting to $\mathbb{R}_+ \times \{0\}$ we get a ppp with intensity measure $ (1-p)\lambda \mathcal{L}$, conversely restricting to $\mathbb{R}_+ \times \{1\}$ we get a ppp with intensity measure $p \lambda \mathcal{L}$. This is exactly a Poisson process with rate $(1-p) \lambda $, respectively $p \lambda $.  	
\end{proof}}

Let $(N_t^0)$ and $(N_t^1)$ be independent Poisson processes with respective rates $\lambda_0> 0,\ \lambda_1> 0$. Define $N_t = N_t^0 + N_t^1$. $N$ is a counting process and we define for every $i$ 
\begin{align}
	X_i = \mathbbm{1}_{\{\textrm{i'th jump of }N_t\textrm{ is a jumping time of }N_t^1\}}.
\end{align}
\begin{theorem}[Superposition]
$N_t$ is a $pp(\lambda_0 + \lambda_1)$ and  $(X_i)$ is a marking of $N$ with 
\begin{align}
	\boxed{	\forall i\quad \mathbb{P}_{} \left[ X_i=1 \right] = \frac{\lambda_1}{\lambda_0+\lambda_1}.}
\end{align}
\end{theorem}
\begin{proof}
	$N$ is a counting process (it follows directly from the definition).
	We consider (independently of $N^{0},\ N^1$) $(\widetilde{N}_{t})_{t\geq 0}$ a Poisson process with intensity $\lambda = \lambda _0 + \lambda _1$ and $(\widetilde{X}_k)_{k> 0}$ i.i.d. Bernoulli $\left( \frac{\lambda _1}{\lambda _0 + \lambda _1}\right)$. By the theorem in the previous section, the thinned process $N^{0},\ N^{1}$ are independent processes with respective rates $\lambda _0, \lambda _1$. For every $t_1 < \ldots < t_k$ and ever $f:\mathbb{R}^{k}\to \mathbb{R} $ bounded.
	\begin{align}
		\mathbb{E}_{} \left[ f(N_{t_1},\ldots, N_{t_k}) \right] 
		&= \mathbb{E}_{} \left[  f(N_{t_1}^{0}+ N_{t_1}^{1}, \ldots , N_{t_k}^{0}+ N_{t_k}^{1})\right] \\
		&= \mathbb{E}_{} \left[ f( \widetilde{N}_{t_1}^{0}+ \widetilde{N} _{t_1}^{1}, \ldots , \widetilde{N} _{t_k}^{0} + \widetilde{N} _{t_k}^{1}) \right] \\
		&= \mathbb{E}_{} \left[ f( \widetilde{N} _{t_1}, \ldots , \widetilde{N} _{t_k)} \right]. 
	\end{align}
	Therefore $N$ is a $ \textrm{pp} (\lambda )$. Similarly, for every $t_1 < \ldots < t_k$, for every $p> 0$, and every $f:\mathbb{R}^{k}\times \{0,1\}^{p}\to \mathbb{R}$ measurable and bounded
	\begin{align}
	\mathbb{E}_{} \left[ f(N_{t_1}, \ldots , N_{t_k}, X_1, \ldots , X_p) \right] = \mathbb{E}_{} \left[ f(\widetilde{N} _{t_1}, \ldots , \widetilde{N} _{t_k}, \widetilde{X} _{1}, \ldots, \widetilde{X} _{p} \right] .
	\end{align}
	Hence $X_1, \ldots, X_p$ are i.i.d. $ \textrm{Ber} \left(\frac{\lambda _1}{\lambda _0 + \lambda _1}\right)$ random variables independent of $(N_{t_1}, \ldots , N_{t_k})$.
\end{proof}
{\color{blue}
\begin{proof}
	Viewing $N^{0}$ and $N^{1}$ as Poisson point processes, the superposition of them yields a $ \textrm{ppp} (\lambda_0 + \lambda_1) \mathcal{L})$, i.e. a $ \textrm{pp} (\lambda )$. 
	Furthermore, $X_i$ is clearly a Bernoulli random variable with parameter $\left( \frac{ \lambda _1}{\lambda_0 + \lambda_1}\right) $. (Maybe here we should use the proof given in the notes? this may be too little).
\end{proof}
}

\noindent \textbf{Conclusion} We successfully defined a renewal process with the Markov property, we also found that this object is also a ppp, thus giving us a process which has the asymptotic behavior (LLN, etc) from the renewal process perspective and getting the Strong and Weak Markov Property from the Poisson Point Process perspective. 


